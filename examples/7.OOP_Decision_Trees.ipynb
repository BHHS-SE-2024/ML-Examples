{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f45ee9",
   "metadata": {},
   "source": [
    "#### [PREV](6.OOP_Neural_Network_Adv.ipynb) | [HOME](../README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd281d",
   "metadata": {},
   "source": [
    "> [!Important]\n",
    "> If you are not using a CodeSpace, before starting this Jupyter Notebook you will need to run the following commands in the bash shell\n",
    "> - `sudo apt-get update`\n",
    "> - `apt-get install graphviz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOP Decision Trees Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will visualize how a decision tree is 'splitted' using information gain. This is a very complex implementation, students are not expected to write implement it, but rather step through the Jupyter Notebook and deepen their understanding of 'information gain' and 'splitting' by seeing it visually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexamples/lib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\benpa\\OneDrive\\Documents\\GitHub\\Machine_Learning_OOP_Implementation_Examples\\examples\\lib\\utils.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydot\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydot'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('examples/lib')\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "_ = plot_entropy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                                                     |   Ear Shape | Face Shape | Whiskers |   Cat  |\n",
    "|:---------------------------------------------------:|:---------:|:-----------:|:---------:|:------:|\n",
    "| <img src=\"images\\0.png\" alt=\"drawing\" width=\"50\"/> |   Pointy   |   Round     |  Present  |    1   |\n",
    "| <img src=\"images\\1.png\" alt=\"drawing\" width=\"50\"/> |   Floppy   |  Not Round  |  Present  |    1   |\n",
    "| <img src=\"images\\2.png\" alt=\"drawing\" width=\"50\"/> |   Floppy   |  Round      |  Absent   |    0   |\n",
    "| <img src=\"images\\3.png\" alt=\"drawing\" width=\"50\"/> |   Pointy   |  Not Round  |  Present  |    0   |\n",
    "| <img src=\"images\\4.png\" alt=\"drawing\" width=\"50\"/> |   Pointy   |   Round     |  Present  |    1   |\n",
    "| <img src=\"images\\5.png\" alt=\"drawing\" width=\"50\"/> |   Pointy   |   Round     |  Absent   |    1   |\n",
    "| <img src=\"images\\6.png\" alt=\"drawing\" width=\"50\"/> |   Floppy   |  Not Round  |  Absent   |    0   |\n",
    "| <img src=\"images\\7.png\" alt=\"drawing\" width=\"50\"/> |   Pointy   |  Round      |  Absent   |    1   |\n",
    "| <img src=\"images\\8.png\" alt=\"drawing\" width=\"50\"/> |    Floppy  |   Round     |  Absent   |    0   |\n",
    "| <img src=\"images\\9.png\" alt=\"drawing\" width=\"50\"/> |   Floppy   |  Round      |  Absent   |    0   |\n",
    "\n",
    "\n",
    "We will use **one-hot encoding** to encode the categorical features. They will be as follows:\n",
    "\n",
    "- Ear Shape: Pointy = 1, Floppy = 0\n",
    "- Face Shape: Round = 1, Not Round = 0\n",
    "- Whiskers: Present = 1, Absent = 0\n",
    "\n",
    "Therefore, we have two sets:\n",
    "\n",
    "- `X_train`: for each example, contains 3 features:\n",
    "            - Ear Shape (1 if pointy, 0 otherwise)\n",
    "            - Face Shape (1 if round, 0 otherwise)\n",
    "            - Whiskers (1 if present, 0 otherwise)\n",
    "            \n",
    "- `y_train`: whether the animal is a cat\n",
    "            - 1 if the animal is a cat\n",
    "            - 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1, 1, 1],\n",
    "[0, 0, 1],\n",
    "[0, 1, 0],\n",
    "[1, 0, 1],\n",
    "[1, 1, 1],\n",
    "[1, 1, 0],\n",
    "[0, 0, 0],\n",
    "[1, 1, 0],\n",
    "[0, 1, 0],\n",
    "[0, 1, 0]])\n",
    "\n",
    "y_train = np.array([1, 1, 0, 0, 1, 1, 0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For instance, the first example\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the first example has a pointy ear shape, round face shape and it has whiskers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each node, we compute the information gain for each feature, then split the node on the feature with the higher information gain, by comparing the entropy of the node with the weighted entropy in the two splitted nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the root node has every animal in our dataset. Remember that $p_1^{node}$ is the proportion of positive class (cats) in the root node. So\n",
    "\n",
    "$$p_1^{node} = \\frac{5}{10} = 0.5$$\n",
    "\n",
    "Now let's write a function to compute the entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -p * np.log2(p) - (1- p)*np.log2(1 - p)\n",
    "    \n",
    "print(entropy(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate, let's compute the information gain if we split the node for each of the features. To do this, let's write some functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(X, index_feature):\n",
    "    \"\"\"Given a dataset and a index feature, return two lists for the two split nodes, the left node has the animals that have \n",
    "    that feature = 1 and the right node those that have the feature = 0 \n",
    "    index feature = 0 => ear shape\n",
    "    index feature = 1 => face shape\n",
    "    index feature = 2 => whiskers\n",
    "    \"\"\"\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i,x in enumerate(X):\n",
    "        if x[index_feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we choose Ear Shape to split, then we must have in the left node (check the table above) the indices:\n",
    "\n",
    "$$0 \\quad 3 \\quad 4 \\quad 5 \\quad 7$$\n",
    "\n",
    "and the right indices, the remaining ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_indices(X_train, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need another function to compute the weighted entropy in the splitted nodes. As you've seen in the video lecture, we must find:\n",
    "\n",
    "- $w^{\\text{left}}$ and $w^{\\text{right}}$, the proportion of animals in **each node**.\n",
    "- $p^{\\text{left}}$ and $p^{\\text{right}}$, the proportion of cats in **each split**.\n",
    "\n",
    "Note the difference between these two definitions!! To illustrate, if we split the root node on the feature of index 0 (Ear Shape), then in the left node, the one that has the animals 0, 3, 4, 5 and 7, we have:\n",
    "\n",
    "$$w^{\\text{left}}= \\frac{5}{10} = 0.5 \\text{ and } p^{\\text{left}} = \\frac{4}{5}$$\n",
    "$$w^{\\text{right}}= \\frac{5}{10} = 0.5 \\text{ and } p^{\\text{right}} = \\frac{1}{5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(X,y,left_indices,right_indices):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    w_left = len(left_indices)/len(X)\n",
    "    w_right = len(right_indices)/len(X)\n",
    "    p_left = sum(y[left_indices])/len(left_indices)\n",
    "    p_right = sum(y[right_indices])/len(right_indices)\n",
    "    \n",
    "    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_indices, right_indices = split_indices(X_train, 0)\n",
    "weighted_entropy(X_train, y_train, left_indices, right_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the weighted entropy in the 2 split nodes is 0.72. To compute the **Information Gain** we must subtract it from the entropy in the node we chose to split (in this case, the root node). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X, y, left_indices, right_indices):\n",
    "    \"\"\"\n",
    "    Here, X has the elements in the node and y is theirs respectives classes\n",
    "    \"\"\"\n",
    "    p_node = sum(y)/len(y)\n",
    "    h_node = entropy(p_node)\n",
    "    w_entropy = weighted_entropy(X,y,left_indices,right_indices)\n",
    "    return h_node - w_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain(X_train, y_train, left_indices, right_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the information gain if we split the root node for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feature_name in enumerate(['Ear Shape', 'Face Shape', 'Whiskers']):\n",
    "    left_indices, right_indices = split_indices(X_train, i)\n",
    "    i_gain = information_gain(X_train, y_train, left_indices, right_indices)\n",
    "    print(f\"Feature: {feature_name}, information gain if we split the root node using this feature: {i_gain:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the best feature to split is indeed the Ear Shape. Run the code below to see the split in action. You do not need to understand the following code block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "print(graphviz.__version__)\n",
    "tree = []\n",
    "build_tree_recursive(X_train, y_train, [0,1,2,3,4,5,6,7,8,9], \"Root\", max_depth=1, current_depth=0, tree = tree)\n",
    "generate_tree_viz([0,1,2,3,4,5,6,7,8,9], y_train, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is **recursive**, which means we must perform these calculations for each node until we meet a stopping criteria:\n",
    "\n",
    "- If the tree depth after splitting exceeds a threshold\n",
    "- If the resulting node has only 1 class\n",
    "- If the information gain of splitting is below a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final tree looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = []\n",
    "build_tree_recursive(X_train, y_train, [0,1,2,3,4,5,6,7,8,9], \"Root\", max_depth=2, current_depth=0, tree = tree)\n",
    "generate_tree_viz([0,1,2,3,4,5,6,7,8,9], y_train, tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "56d44d6a8424451b5ce45d1ae0b0b7865dc60710e7f74571dd51dd80d7829ee9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
